Podcast: AI Agents and Multi-Agent Systems with Victor Dibia

All right, everyone, welcome to another episode of The TWIML AI Podcast. I am your host, Sam Charrington. Today, I'm joined by Victor Dibia.

Victor is Principal Research Software Engineer at Microsoft Research. And we've got a great conversation lined up for you today. We'll be reviewing Victor's take on the most important AI agent innovations in 2024, and what we should expect to see in the year to come.

And of course, we'll also discuss Victor's work on multi-agent frameworks in general and AutoGen in particular. Victor, welcome to the podcast.

Thank you, Sam. It's great to be here.

I'm super excited for the conversation. I know in your world as in mine, I guess more so in your world even than in mine, agents is a hot topic that comes up all the time. And I know you've got a lot of interesting takes on that topic.

Let's get started by having you share a little bit about your background.

I'm a Research Software Engineer. I work at Microsoft Research. I work specifically with a group called the Human AI Experiences Group.

And essentially by design, we are interested in scenarios where a human works in tandem with an AI model to solve tasks. In terms of background, my training is mostly software engineering. Some work in HCI.

So I have a master's in computer science, information networking from Carnegie Mellon University. And I did a PhD in Information Systems at City University of Hong Kong. And my PhD is mostly focused on human-computer interaction, user behavior psychology, and how we can conduct a set of experiments that help us understand how people make decisions as they use technology tools and interfaces.

And the whole idea is that we take all of that knowledge and we sort of apply it in designing better interfaces.

Your group at Microsoft sounds like a traditional HCI research group, but you ended up building AutoGen. And I don't know if you own that as a product. I guess my point is it feels very productized as a software infrastructure product coming out of this HCI group.

So how did that all come about?

Yeah, so I could talk about my own personal path to agent, and then I could talk a little bit about the history of some early stories around AutoGen. So I started out right after my PhD. I started out as a postdoc at IBM Research over at New York, Yorktown Heights, and then I stayed on as a research staff member.

And one of the things I worked on there was I was with a HCI group, and we worked very closely with a core machine learning group. And at the time, IBM had just come up with the cognitive service APIs. And we were building all this complex multimodal demos around like speech to text, text to speech, image recognition.

Using all of that together in like physical room scale experiences. One of the things I did back then was that I trained perhaps the first model for automated data visualization. So I didn't actually remember the sequence to sequence models.

And so they were typically used for language translation. And they were like the state of the art back then. And we showed that if you could represent visualizations in JSON, VegaLite, and you could represent like data in the same JSON specification, then you could learn translations across the two.

And so at runtime, we gave this system, this model, like some text. We sampled a couple of rows from the JSON dataset directly. And it will generate a bunch of visualizations that were grounded on that.

So that was really interesting. And if you think about it, there's a bit of action there, right? And so we generate like Vega specification, we compile it, and we give a visualization.

And so after that, I spent some time at Cloudera, traditionally a big data warehousing company, but it had like a machine learning group, and we built a lot of prototypes, did some customer consulting. Then after that, I joined Microsoft Research. And there, I did some work on a tool called LIDAR.

And so LIDAR is again, an automated visualization tool, but had like a bit of a pipeline. So first of all, we got some data. We got like an LLM to generate a summary, an enriched summary of this data.

Based on that, we got an LLM to generate a sort of hypothesis that made sense for this data. And then for each of these hypotheses, we could get an LLM to write code, and the background, we did like process and post-processing, we got code, we executed it. And we gave folks a bunch of visualizations.

And so, and this was in 2000, just pretty early, I think, late 2020, before ChatGPT. So it was an entire interface, you know, that did all this like pipeline work on the backend. And it used the codex set of models.

And for the most part, you can see some agentic behavior there. And so it's not just LLMs generating stuff, you know, we're compiling code, we're doing pre-processing, post-processing. And so once you create a lot of these pipelines, you start to see like some broader patterns.

And so, you know, the next question is, can we go from that to a system that like, without having to manually sort of build out the exact steps in the pipeline, instead, we define the set of agents, and we get them a task, and they sort of collaborated autonomously to sort of solve this problem. And one instantiation of work like that was AutoGen. And so for the most part, AutoGen started out like exploring this theory that maybe we could explore a new way to develop applications.

And so instead of building specific pipelines to express the problem to express the solution to a problem, we could instead define a set of agents with fairly broad capabilities. We give them a task and they could sort of collaborate to solve a problem. And there was a really, there were a few really clever people within our broader group started to explore this, did a bunch of experiments, wrote a paper.

So I wasn't an original author on that paper, but we worked very closely, I worked very closely with that group. And essentially that's sort of like what led to AutoGen as a framework. And it's been about a year and some months, a lot of things have happened.

We've got a bunch to dig into here, and I don't necessarily want to belabor things by talking about like defining agents, but you mentioned that you heard me talking with Chip on that topic in a recent interview, and you had your own take on how agents are defined. I'd love to have you share that.

Yeah, so I think a simple definition works. I think a lot of people are converging on the idea that like if we take an LLM and we give it access to tools that let it take some action, so essentially this LLM can now act, then we have an agent. And from the software engineering point of view, that's like the basic instantiation.

So you take an LLM, you give it some tool calling capabilities, you give it the ability to execute the results of those tool calls, and then you have an agent. And I feel like I'm happy with that base definition. In practice, it can be, if you want it to be a bit more precise, I think there might be a few other things.

So you want something that has the ability to reason, has the ability to act, know their tools, has some adaptation capabilities, in this case, perhaps memory, and then finally some abilities to communicate, and so it can send messages to other agents or to humans. And so it's reason, act, communicate, and adapt. And so this is, I think, like the four built-in blocks I would say sort of make up an agent.

It seems like that reasoning ability is a key differentiator between a traditional software system that uses large language models and an agentic system in my mind, and that it is what unlocks the ability for it to be dynamic as opposed to like statically defined workflow.

Yeah. So reasoning, yes, I do agree. I think one way to think of it is from the perspective of, let's say, planning.

And so you get a task, you decompose it into a set of steps, and the idea is that if you succeed at executing each of the steps, you go from a state where the problem is unsolved, the task is unsolved, and then you arrive at a step where the task is now solved. And I think you touch on the idea of dynamic. I think the interesting bit here is like, do you, you know, as you take each of these actions, you might, you know, the problem might exist in a dynamic space or a dynamic environment, and each time you take an action, it changes the environment, and in some cases, those changes might lead to errors or failure conditions.

And I think the key part is like a good, like autonomous or good multi-agent system should have the ability to sort of recover from that and sort of either abandon the current plan or make adjustments and sort of keep making progress. And a really simple example is that, let's say you're trying to solve a problem, your agent writes some code, executes it, there's some errors there, it looks at the code, based on the error, it sort of modifies the code, executes it again, my missing libraries incurred arguments. And if you do have this sort of behavior where, you know, every action has some results, some outcome, and you can respond to that and then keep making progress, then I feel that sort of speaks to the dynamic aspect of a multi-agent system.

So we're going to dig into that, I think, in a lot more detail. But I think for this first part of the conversation, you put some thought into kind of what you, you know, from your perspective, the most important developments in agents over the past year or so, and, you know, and how those set the stage for the upcoming year. And I wanted to start by digging into some of those.

So I think the first thing on your list is about adoption.

Yeah, let's start right there. So over the last five months, I figured out it would be great to sort of keep track of what's changing. So what I did was that each time I saw like a research paper or a new product or a new tool, I kept a bunch of notes.

And at the end of the year, December last year, I sort of figured out what are high level categories here. And in terms of adoption, I feel like a lot of enterprises and teams adopted the 10 agents, but they did that with like some caveats. And so for the most part, what most people deployed last year or in the last year was mostly an LLM as a thin wrapper around existing APIs and tools.

And so as opposed to like fully autonomous behavior where like at runtime the agents can explore on unknown or like unscripted paths. Essentially, they just took the existing APIs, very, very tight, structured action space, and all the LLM can do for the most part is to make calls to these APIs. And this is a really good game plan because you get a lot of reliability out of that.

I think the second thing on that little list was the rise of agent-native foundation models. And so about a year ago, we mostly had like models like GBS 3.5 and the equivalent and Google from Google and Anthropic. And most of these models mostly focused on language modeling.

And so they were writing text, they were writing code. And for the most part, they were mostly text-in or in some cases, multimodal text-in, image-in, but only text-out. And one of the things we saw in the last year was that these models were increasingly integrating multi-agent capabilities just baked right into the model.

So some of the capabilities like the ability to reflect things. So if you remember the React pattern where the idea is like you get the LLM to come up with the thoughts, get it to reflect in that, and then take more actions. And so we're seeing that like with things like the O1 model family, the ability to just think and reflect is sort of just lifted up into the model itself.

And you know, you give the model a task, it does all this internal introspection reasoning before you get like a result out. And also we saw things like natively multimodal in and out model. So I think the Gemini 2.0 model, so these things can take in text, image, video, audio, and the same model can sort of spit out results across all three modalities.

And so I think that was the second interesting thing we saw in 2024. The third thing had to do with interface agents. And some other people have referred to them as computer use agents.

And the idea is that as opposed to agents just calling tools, APIs, or code, we now see sort of a shift towards agents that act by simulating what humans do with interfaces. And so examples of that are agents that sort of solve tasks using a browser. And so I think about two days ago, we saw OpenAI release the operator agent.

And the whole idea is that you could tell things like, you know, book a flight for me, and I'll go to, let's say, flights.google.com or click around, put in all the information, dates, source and destination locations, all that stuff, and then probably come back at some point, get some feedback, get some confirmation, and get things done. Fun fact, on the AutoGen land, we've built out systems or tools like this. And I think one of the excitements of the last two days was that once the operator came out, we said that he has like 40 lines of code “and you could implement your operator using AutoGen.

Did the browser control framework already exist in the AutoGen world?

Yes, that's an excellent question. So in AutoGen world, we have a bunch of presets. And so we have like a preset assistant agent that like, it's a classic.

It just has an LLM model, a set of tools. But we also have this preset called a web surfer agent. And underneath this agent drives a Chromium web browser.

And it uses a multimodal, any multimodal LLM model. And so essentially, it has an action space about how to get work done on the browser, text input, clicking around, navigation, all of that. And essentially, it pretty much just acts by sort of driving and manipulating this browser.

And it's a really nice, well-designed agent was done by one of my colleagues, really brilliant fellow, Adam Phoney. And essentially, all you have to do is plug in this preset into your multi-agent team and get all of that capabilities. Nice.

So interface agents?

Yes. So interface agents. So we have that with AutoGen, the web server agent in AutoGen.

We also have like tools from Anthropic. You know, they have like a computer, computer use implementation. And there are a bunch of other like tools that sort of exist in that space.

And so we saw a few of those sort of like advancements in 2024. The third thing had to do with complex tasks and frameworks. And so I did see that, you know, as a community, as a field, line chain got us very, very far.

So line chain showed how you could sort of get a set of deterministic steps, put them together in chain, execute them. But, you know, there was a bit of appetite for more complex workflows. And we, essentially, more autonomous kind of workflows were like the task.

You want a system that can address any task. And in fact, it reminds me of an article that, like, Bill Gates wrote about, I think, a year and a half ago, talking about how today, you know, back then, if you wanted to sort of, let's say write an email, you went to like an email processing app, Outlook. If you wanted to do CRM stuff, you went to a CRM app.

And if you wanted to do music stuff, you went to a music app. And he talked about the idea of an everything app, a unified interface where you just expressed your task in natural language. And the system just, if it needed to manipulate or reach out to other systems, it did that.

And so I feel there's a lot of value, a lot of time-saving, effort-saving value proposition there. And I think the community sort of started to resonate around that. And I think organically, that has led to the design of frameworks like AutoGen, LandGraph, Korea AI, Lama Index, because we want to figure out ways to provide good presets that help people build this sort of, like, generalist kind of systems.

And I think, again, I might touch on what I mean by complex tasks, but I think that's one of the shifts that we saw in 2024. So beyond scripted deterministic pipelines to more autonomous, like, systems that could sort of address multiple disparate tasks. And then the final, the final, I said, the final update had to do with moving beyond just benchmarking models independently, but essentially extending to just end-to-end, like, evaluation of agentic systems on tasks that require action across multiple domains in the real world.

And I think one of my favorite, one of my favorite benchmarks day is the Gaia benchmark. And essentially, I think, if I recall correctly, it's about 300 problems that, as at the time of release, it looks really simple. These problems are really simple, really easy for humans to accomplish.

But the best models at the time, I think, was GPT-4, which just feels really bad. I think they had like a 10% pass rate there, if I recall correctly.

What are some examples of the Gaia tasks?

Yeah. So it might be things like, how long would it take Aliyut Kipchagui to run across the earth, let's say, 50 times? Now, to do something like that, you need to figure out, oh, who is Aliyut Kipchagui?

What's his maximum? He's a marathon record holder, so you need to figure out what's his speed. And if you are, you know, what's the circumference of the earth, then you need to do that little math that sort of puts everything together.

And it might be things like, you know, like what did Sam Charrington say in the 78th minute of his 2025, I don't know, January 1st podcast? Now, to do that, you need to go to YouTube, find like who's Sam Charrington, find the exact YouTube video that's being referenced, extract, extract the transcript, and go to the 78th minute and figure it out. Now, as a human, this is really straightforward, frankly, but how does a machine go about stuff like this?

If you really think about it, there are all kinds of ways where this machine might fail. And I remember a group, again, led by one of my colleagues, developed a generalist agent system called “Magentic One. And essentially, for a long time, it held the state-of-the-art performance on tasks like that.

And all of that process was extremely instructive. We learned a lot about how these things could fail, the differences between how humans think about problems, when machines, even the ones driven by sophisticated algorithms, try to address the same tasks. And so, I think that was like the third and more interesting, the third interesting update for 2024, or the fifth, sorry.

So, let's dig into these. I have a bunch of questions across this list. Maybe let's start with these kind of agent-native foundation models, you call them.

Talk a little bit more about the way you think of them as agents. I think, I guess my personal experience is that I originally thought of them in a very agentic way, but then, as we've seen with DeepSeek showing you the thought tokens, it seems less agentic, in a sense. Does that make sense?

I guess it's like, it seems more like a straightforward but slightly more complex application of traditional LLMs in some way.

Yeah. So, I guess what you're hinting at is like, if you didn't see the thought tokens, then it looked like it was doing something more clever. But when you saw the thought tokens, it was just an autoregressive model.

So, just predicting the very next token. So, I guess the interesting thing is, what is different when the model is primed to explore like the iterative thinking process, as opposed to just generating the next likely token. I think from the human behavioral psychology perspective, and I say this with caution, LLMs are not humans.

They're not like humans in any form. But if you think about it, there's the whole concept of the system, one thinking, system two thinking, thinking fast and slow. And there's a whole idea of like, for things that are simple, as humans, we've adapted to use heuristics, right?

So if I did ask you, Sam, what's your birthday? You don't need to think about it. You tell me.

Or like, what time is it? Or is it morning or evening? You know the answer to that.

So you can rely on heuristics. So these things are like right there at the top of your mind. But if I did ask you like, hey, you know, like the question earlier, how long will it take a marathon runner to run around the earth like 50 times?

Now this requires a bit more investment, a bit more computation and effort. And a lot of people did complain earlier that like, say about a year and a half ago, if you ask the model these two questions, you'll take exactly the same amount of time to give a response. And there's just something not right about it that like, some two problems, so different, so complex, we are assigned about the same effort.

And I think a lot of that has informed some of the work in test time compute. And the whole idea here with the O1 reasoning and DiffSig family models is, we want a way to communicate to the model of the system that like, some problems perhaps require a bit more investment, more computer investment and all this. And it turns out that it does work when you design the system that way, you just “get better results.

Underneath is still an autoregressive model doing autoregressive stuff. But it just turns out that the setup, the problem setup sort of results in better results for thinking and reasoning style problems.

And so the advantages of those types of models for, you know, like complex information gathering and presentation, report generation, those kinds of tasks is pretty clear. Are you seeing those reasoning advantages play out in terms of, like, the planning style of reasoning that's important in making complex agentic systems work?

Yeah, yes. So one of the good things about, like, let's say a tool like AutoGen is, let's say, you could decompose your problem and express them as agents. So you could have an agent that's explicitly just focused on planning.

So, for example, I mentioned earlier the Magentic 1 paper. So the way that that setup was done was that, like, we had, you know, we argued for the design of a generalist system that can address multiple different types of tasks, and we tested them across multiple agentic benchmarks, the exact same system, so nothing was fine-tuned for a specific system. And they were composed of four agents.

So the first was an orchestrator or a planner. So all they did was they took a task and they would decompose it into a plan and assign steps in the plan to other agents. And there were, I think, four other agents, something called a coder.

All they did was write code. There was one that was a computer terminal, all they did was execute code. There was a web surfer agent.

Essentially, the task needed like interaction with the websites. And then there was a file surfer agent. So if you need to open things like video files and image files or PowerPoint presentations, that sort of thing.

And the core idea is that for, let's say, the orchestrator, you could, for each of these things, you could design them different models. And so for the orchestrator, I think my theory is that something like that, that's meant to like reason through the problem, do some sort of task decomposition, assign steps to different like agents.

An agent like that really would benefit a lot from like some of these sort of like test time computer reasoning models. Now, the other is something like, let's say, file software “, all it does is just has a bunch of tools that lets it interact with files. Probably not a lot of benefit there.

And is that intuition, or have you seen benchmarks that, you know, take a system like a Magentic One and insert a reasoning agent for that orchestrator step?

We haven't released any results. Let's say, let me use the word release. We haven't released any results yet, but early experiment and some of, I think I don't remember other papers off the top of my head, but I think I have seen a few where just dropping in the reasoning model did give like a significant boost.

I guess I want to poke at like there's maybe a nuanced difference between seeing a significant boost and like unlocking a whole new area of capability. Do you see these types of models doing the latter?

I wouldn't see exactly a whole new, like a whole new type of capability. I think it's just maybe performance improvement. I think a lot of the types of problems we're thinking of solving with this system is still the same class of problems.

Maybe, I think the more interesting thing here is, if we look at this system from the perspective of like failure modes, bad plans, or the ability to come up with good plans on the first try is a significant performance issue for this sort of autonomous systems. And from that perspective, if you get something that reasons well, comes up with a good plan on the first try, then you get some benefit there. But the type of problem hasn't changed.

It's not like we're suddenly, I don't know, we're suddenly doing new types of things. It's just that we're getting reliability or performance, maybe even safety improvements where possible.

So with the Magentic 1 work, you mentioned that one of those agentic types was a coder. Was that primarily used in the context of coding problems, or was it code that was generated in the process of solving other types of problems? I've come across several different papers that use code as this intermediary for planning and other things, and I find that a really interesting and compelling way to use code.

Yeah, that's a really, really good question.

So it brings me to how I think about tools. I think there are two types of tools. So there are tasks specific on other domain tools, and then there are general purpose tools.

And I feel like a code interpreter is a type of general purpose tool. Some problems, a lot of problems can be... The solutions to a lot of problems can be expressed as code.

And the key point here is getting the orchestrator to figure out, okay, this support problem could be solved really well when expressed as code. And then getting the coder to sort of write that code, and then getting the code interpreter to execute it is like an emerging pattern. So to answer your question, it wasn't just solving like software engineering task type problems.

It was mostly like, hey, no, he has a task.

There's more code interpreter than code generation.

Yeah, yeah, that would be like a good focus. And of course, there are caveats there. So if you have a system that has this very wide action space, then it can do a lot of interesting, maybe even unusual things.

So an example that we, a funny example that we like to talk about, like in the margin, and we talk about it in the margin, took one paper. At some point, the agents were looking for some information. They were supposed to conduct a Google search, and they failed to find that information.

And you can imagine what they did. They wrote some code to send an email to request an FOIA, essentially to send freedom of information, and emailed that organization to request that data. It's like, hey, we're conducting this research.

We need this information. We can't find it. By law, we're supposed to have access to it.

And they crafted this email, and they were going to use an email API to sort of send it.

Imagining the agents sending it to, like trying to send it to Google, as opposed to a government organization or something.


Yeah. So, it's a fun fact, but it is true if you don't constrain the action space of what these models can do, because code is just this really expressive thing. They can take any kind of action, express it as code, and then it could lead to things that...

And the way you solve this is that the orchestrator can make some high-level decisions as to, is the task being stalled? Is it going the wrong direction? Sort of metacognition, right?

As these agents act, the orchestrator is sort of inspecting the progress and is saying things like, you know, are we stalled? Was our maximum stall count? And should we reset, modify the plan, abandon this route, and take a separate route?

So, yeah, that caveats to using, like, general purpose tools.

And the way you talked about those aspects of the orchestrator is maybe a segway into talking about these complex tasks and frameworks, which was one of your items. Specifically for those types of parameters you were describing, are those things that the user of a framework like AutoGen, like, are they thinking about them? Are they setting parameters?

Are they coding them? Like, how do you manage the level of abstraction that someone working, you know, trying to build an agentic system to tackle complex tasks has to deal with?

So a question like this, you know, ties into, like, slightly how do you design frameworks? How do developers think? And I could tell you a little bit about how, like...

As an HCI guy, I feel like I've opened the box.

Oh, yeah. And I could be a bit more practical to tell you about how we are pushing with AutoGen. And so in AutoGen, there are currently there are two API levels.

So there's a core API, and the idea is that, like, it mostly just provides you with the bare bones capabilities for things like just message delivery. And so anything could be an agent. You could define anything as an agent, inherit from a base class, and the only thing you're required to do is to modify a method that says, you know, the agent has received a message.

What does it do? And so it could be as simple as it receives a message, it does nothing, or it sends back the exact same message, and that's all. No opinion to whatever the agent does when it receives a message.

The developer is responsible for that. And essentially, all we guarantee is that there's a concept of a runtime. When you define your agent, the runtime spins up, creates instances of this agent, enables message delivery, and this agent might live across multiple machines, they might be on the same machine, and that's all.

But for most developers, this is still too low level. And so we have another API called Agent Chat. And Sam, you're from the old world, you probably remember Keras.

Well, Keras was like this high-level abstraction, very intuitive, but beneath it could run a TensorFlow backend, or a PyTorch backend, or a JAX backend. So think of Agent Chat as the Keras of this world. And the kind of presets we have there is things like a basic assistant agent.

And so this thing is what I think is the fundamental representation of a basic agent. And so it can take an LLM, it can take a list of tools, it can take a list of memory banks, and essentially that's the standard interface, that's the standard definition there. And so model client, a list of tools, which could be functions, it could be anything, and a set of like memory banks, so just to enable a rag or like just-in-time retrieval of the information.

And we have another preset, which is like the web server agent, which essentially is just all the things you need to drive a web browsing and sort of accomplish tasks using that. And then we have, I think, one or two other presets, not very important. So that's at the agent level.

Then how do these things sort of collaborate? So we have the concept of teams. And so think of them as containers that you put these agents into.


And it mostly governs the order in which messages flows across these agents. And so we have a preset, something called a round rubbing team. And what it does is that once a task comes in, it just sort of sends messages across each of the agents until some termination condition is met.

And then the final abstraction we have at the team level is a termination condition, which can be really, really tricky. It's like this guy said, exploring a task, how do they know when it's done? And so we have abstractions like text message termination.

So means if any of the agents, you could define in their prompt their behavior, they might constantly sort of inspect the state of the task. And if the task is done, they might respond with a word like terminate. And so let's say you scan for that in the messages, you decide that things are done.

it could be budget based so timeout based or a maximum number of tokens used or maximum number of steps um it could be some external signal so something external just monitoring the state of the task and then sends like a termination condition and you can compose all of these things in all and combinations and and if you took all the presets agents with all the stuff inside teams round rubing uh selector group chat graph based like selections all of that and termination conditions and you put all of that together um we are saying that that has been like a powerful way to sort of Express like autonomous multi- agent systems popping up a level um you started talking about kind of you know the way developers think about problems and things like that and it sounds like at its core the way autogen is organized is kind of message based and there other approaches that are graph-based other approaches that are um you know I don't know of specific other ones but there seems to be like you know message and graph is like one big um you know difference in Paradigm you know are there others and um you know why do you think message is better you know historically like or traditional software it's like loose coupling is an advantage of message as opposed to other things like talk us through that whole abstraction uh you know thinking I I'll talk about two things so first of all right now the current version of autogen is based on the nage passing like um architecture and how we got there and the second thing I'll talk about is like some of the emerging patterns I I alluded to some of that but I'd like to structure it a bit more emerging patterns we're seeing for building the sort of autonomous multi-agent systems so about about a year and a half ago when we released the first version of aogen um the interesting thing is you know it was built on this idea of conversational programming and so the idea is that like to solve a task we just get this agents to each act so each time they act they have like a shared conversation history or list and llms were actually being fine-tuned and they still this still a paradigm know the chat completion like Paradigm where like every time you ask a question you appended to like a long chat and the the model just gets no it's it's been trained to use context is coming from yeah exactly all the all that context so essentially uh solving tasks is just all about building context that just like extends but the problem was that this thing was on a list that lived in memory and so all the agents had like this list literally like there this list and they all like appended stuff to it so each time they executed code or they responded to a message they all append uh sort of append into that list of memory now if you do that you can build systems where like the agents leave on multiple machines and this is a really common production requirement like you want the agents Liv in a separate machine they have like security boundaries have access to information that like nobody else should be able to have access to but you know if your design is that like everything leaves in memory you can do that um in in in addition to that um you also want an asynchronous stack because each time the agents Act an action can take an abitrary amount of time and you want a scenario where like um these things can run in the background while the rest of the system sort of continues and so quickly we ran into all of those sort of issues and a solution to that is the actor model and so you sort of reference that where know you you treat um every element in the system as an actor and they're Loosely decoupled from everything else and only um they only communicate via messages and you send this asynchronous messages the messages can be delivered in arbitrary order and if you do that um you can compose this message sending Behavior into all kinds of complex patterns um you can have like agents or systems that like leave on multiple machines as long as they can connect to the same like communication layer or message passing or message deler layer and even from the application development point of view um you can build this async applications websites uis communicating with things like teams slack where like know messages are just sent and received um asynchronously so um things like you know enabling true like distributed agents it's one of the uh driving principles there and also just application just better like integration with external applications are one of several benefits that you get from um a message driven sync acto model kind of Paradigm so hopefully that sort of provid some background and why like message passing um is a is a good idea of course it's it has its own complexities uh message delivery ordering um it's really hard to think about and debug async code um but that's what a framework is there for meant to help with a lot of this those issues um so that's that's one thing um the second part I want to talk about is you know I mentioned the idea of graphs and chains and all of that so I feel like there are two high level patterns that we are sort of observing in the multi-agent space so the first is control flow patterns and so if you have multiple agents how do you determine the order in which the act or the order of message delivery across each of these agents and the simplest vers version of it is a deterministic chain where like you say you know I want something that like generates like I don't know finds me some news articles every day so you might just keep it simple it has three steps you have an agent that like takes in my query it takes in my requests generates like a web search query you have another one that makes a request to being a Google and you have a third one that like downloads all of the data and all the results and summarizes it into some simple outcome uh extremely simple a simple chain like that is also a graph but just a very simple version of a graph and the core idea here is the developer already has a clar idea of what exactly what they want the system to do when you run this graph um now the graph could get a little bit more complex which is what tools like line graphs supports so you enable like conditional edges you enable loops um by the end of the day you can still make very clear deterministic predictions about what would happen before you execute the graph so you know that this graph is directed is a set Le and it will always come to some end goal or one of several end goals which are all good things so that's like the the first two patterns so simple chains graphs and then the third piece um is more implicit planning or group chat kind of thing where um we model the solution to the problem not as graphs but just by a conversation history which is where other started and this agent sort of um no there's no no there's no predefined path but there are some structures around like you know we might have some round rubbing communication flow or we might get an llm to decide just in time which agent speaks next or takes another turn um and I'll say that's the the the second plan and this implicit plan group chat shared context kind of thing it can still work across like multiple distributed agents it just is a bit more nondeterministic and we still see a lot of failures and I think as a research group um that's a part that's really really interesting because like we want to get a point where we understand this thing really really well and make it work for everyone because this is how you get like truly autonomous like new Step function like increases in system Behavior at least in my my in my opinion and then there's the opportunity to implement things like meta cognition to just thinking about thinking um and so I mentioned magentic one an early version of that there's a concept of an inner loop and an outer loop and so every time an agent takes a step just the the orchestra sort of takes a pause he you know is a task making progress um if yes okay let's keep on going if no we we increase some stall counter like hey you know we stalled and after some threshold we just reset all of the agents we agreed that like we are on a bad trajectory we we mightbe summarized what went well we keep it we use that formulate a new plan with some notes and then we sort of explore new trajectory so a bit of medical recognition reflection that kind of thing and then the final set of patterns are around task management um um around like first human delegation and so not all actions are equal and so if there was if if an agent came and said things like hey I'm going to download this file to disk um no big deal right um it downloads a file but if it said something like hey I'm going to send um I don't know I'm going to transfer some money to Sam charington account using a now I want to know about that right right right I want to know about that so there's the idea of like I want to know about that too oh yeah you want some to know about it right so he doesn't get worried like hey you know where did all this money come from so there's the idea of like the inherent risk associated with actions so we need to be able to quantify that and maybe figure out if we want to delegate and also there's the idea of um how do we know when the task ends um and and I I've hinted hinted on that earlier so those are like the two high level patterns control flow and task management patterns that I'm sort of seeing kind of putting on the hat of someone who you know is thinking very pragmatically is you know maybe at a startup and is building something or wants to build something is it too reductive to translate what you said or a little bit of what you said into like autogen is a research project and you know they're doing things the way to make it you know interesting and complex for research and open up new avenues of research and you know I might want to go a different direction if I am really just trying to solve a problem so maybe six months ago that would have been correct um but um I think two weeks ago we released a new version of aen if you search for it I I know this is a little bit confusing for some people who have used aogen but there's a new version called aogen 0.4 and it's based on this new like asynchronous um message delivery uh Behavior I mentioned earlier and we've put a lot of effort there to make it production ready so like I mentioned there's a core API right and so I'll say my my suggestion is that if you're start of trying to build like a production level um application just take a look at the core API um there's a chance that like your business logic is so Niche that um maybe someone the presets we have in the higher level API might not be directly or immediately applicable but um take a look at the core API use it as a core building block because at the end of the day you you still need like um message delivery capabilities you need some of the utilities around Tools around like model clients that sort of thing um so I'd say the core API is where you want to sort of invest time in and also another fun another fun fact that I probably will tell you is that when we built autogen you know again as a research group we're more interested in autonomous behavior and this is a good thing everything starts out as research somebody has to like explore it before like we solve all the bugs and make it work and so yeah and so we started out with the auton thing but then we saw that like this is just my own assessment but like I think about 70% of the people who come here to use autogen they already know what they want to do they know like my problem has like six steps and essentially what they're trying to do they're trying to shoehorn aen to just do that six steps that they um they're there to do and we're working very hard to make things like that possible and these are all great ideas I think a lot of the reality is just that a lot of the use cases so for a startup right let's say you're in finance you know exactly what the problem domain is is like it's pretty structured you're not trying to build the everything app right and so from that perspective what you want to build is some type of workflow or pipeline or chain or graph maybe with some Dynamic Behavior here and there um as OPP put something fully autonomous and so go ahead try out the core API use it to express your business problem and I think that's really uh where you get the most benefit from um and as the space grows and we figure out like really how to get this autonomous thing to work really really well um then maybe at that future time um the higher level API or the autonomous exploration kind of agent might be a better fit I think just being pragmatic this would be my my thought process here going back to you know our review of of 2024 uh you mentioned interface agents and computer use I think that is you know really starting to capture a lot of kind of maybe energy or imagination is maybe even a better word because like you can like really visually see the computer like doing tasks that I don't want to do book My Flights you know do my grocery shopping um I'm thinking a little bit of a another recent interview with Dan Jeff where he talked about actually how hard it is to do those kinds of of tasks um you know he used an example of like trying to get an agent to book a flight on Google flights and like pulling up the calendar and you've got to click the numbers and like getting the agent to get you know localize on those numbers is just really hard like I think you know to to maybe kind of open up this part of the conversation I'm just curious like your take on you know where we are with regard to computer use and um you know what what needs to happen to evolve it to for it to be like how practically useful is it you know in your opinion like just kind of what's your rundown of like the state of play with regard to like agents controlling browsers so if you've ever done like a user study where you get like people to get you get people to use this agent accomplish task um one very funny thing that tends to happen is that there something really unsatisfying about a human just sitting and watching an agent struggle is like okay I'm going to open this browser then I'm gonna click this I'm gonna click that the the VIS on some parts some people like hey that's cool that's really fun but some of people like hey you know like I could do that in like quarter of the time you know that's sort of thing and then sometimes just like you mentioned like it feels at some funny like step and then it has to do and all that so um I I I I think this is all growing pains as a researcher I have very high tolerance with stuff like this of course as a product designer this is not the case um but I think I think there are two things right so we seeing like specialized models just just for like just UI interaction so some of my colleagues Ela at Microsoft research released the model I think couple months months ago something called Omni parer and essentially it's a multimodal model and it's just fine tuned to predict the bounding boxes with fairly high quality of interactable elements on any screen in UI um and again if we treat this as an engineering problem as we gather more data about like how humans like sort of interact think of it say like all the good work that open ey and code did on supervise fine tuning so you just invested a ton of time in just figuring out like you know getting a bunch of experts to interact with the GPT models assembling this sfc data or for reinforcement learning from Human feedback I feel like we are going in that direction where like these models develop intuition similar to how human beings sort of act and so there some things you do if you see an ad you close it immediately before you do anything so like you don't you don't try to like click other things while an ad is open you know oh that's an ad I got to click it it's a cookie popup banner I got to get it out we and so we need more we need more of those types of examples um some icons are really small um there's some models optimized for that I think also about a week ago there's a new model from ban something called UI tars and also just sign ific I I I see a lot of progress so the benchmarks are just just getting better and I think we'll see more of that um so and models like that they they have to specialize like traditional just generic multimodel model models into that well there so just having specialized like UI interaction models is becom a thing so I think that's how we we get better there um I think there's also a lot of work around like memory and adaptation and so we need to figure out which to get the model to do things like remember preferences automatically um say anytime we give feedback or um anytime like it explores a path that seems like it works well um we need ways to sort of to serialize and recover and retrieve that sort of know successful trajectory in the future so um short short answer I I think that you know accuracy will increase um latency will still be a problem and from that perspective I I doubt that the right model to use these things is to have a human sit and watch um I think you really want this thing doing back office stuff like you know um I I I am a I don't know I am a customer sales consultant uh a new ticket comes in just without me being there I want this thing to like open up Salesforce open up our custom I don't know ticket management software do all the data transfer open up an Excel sheet write all the things down and then just let me know when it's done um I'm not sure that the right model is that like know the human actually actively supervises it um maybe there could be like one training or one like learn by demonstration fees and then every other thing should be like Serv in the background um and the truth is if if an agent takes a minute to do something that I can do in 30 seconds um I think this is fine because 30 seconds of my time is worth a 100 times of 30 seconds of or one minute of the agent times I think people get hung up on that um know 30 m 30 seconds of your time is really really a lot worth worth a lot more than than all the computer that an agent needs to use even if it uses just slightly more time so yeah yeah yeah I used to kind of think that and maybe it was more true than when you know when we were further from seeing computer use and things like this in a while but like you know for any problem that was you know sufficiently valuable enough to you know point an agent at it um you know you may as well just like build the API integration or something like that your Salesforce and ticketing example is a a great example um but you know it's it's clear that as you know the cost and capability and complexity of like deploying an agent that works through a web browser um you know goes down and they become more reliable like there are way more integration problems out there than we have the ability to tackle all of them so y there's certainly a place for this back office type of work that you're yeah uh describing even though it's you know as an engineer is painfully less efficient than just like hitting an API yeah I I completely agree with you this whole thing is an an anti pattern really it's a there's something there's something not right the the least efficient way to solve this problem yeah um yeah I agree with you I think maybe reduction in costs and maybe increase reliab ility and that they added flexibility right um there just not not enough like the the surface of all the things you need to build apis for um it's just it's pretty large so yeah yeah I guess another take that I've had is like that you know historically a lot of integration problems aren't like technical problems they're like political problems the example I always use is like you know forever you couldn't get Southwest flights on Google flights and that's not because they couldn't integrate them it's because Southwest didn't want their flights there um do you see um do you see push back starting to happen you know on agent systems like you know in the traditional like in the search engine world there's robots. Tex like you know keep your Bots away from my my content uh you know do you see a thing like that starting to uh or do you see a future in which people are trying to prevent robots from well I guess they're already trying to prevent robots from accessing their sites but like how does it I guess how does it change um given the kind of technology that uh you know gentic Bots are based on yeah you bring up a really important point and I think it's something that we all should well I mean I I guess techn from the technology standpoint we'll all adapt to it in some way but um I feel I agree with you at some point we'll have something like agents. txt just have like have robots. txt and and it might be things like hey know we don't want robots or interface agents like um sort of interacting with any other our any of our interfaces or it might be just a specification for what we expect agents to do or the right way to sort of interact with um with our with our interface and and for that matter you know beyond agents. txt like you know we'll have Cloud flare and you know all the other services that people use to try to prevent um you know non-human access to their to their things oh we already seen stuff like that we already seen a lot of websites they just try to Auto detect if you're using a play right instance or know if it's actually human and essentially the website is just just blocked completely and there's a chance that like a big chunk of the internet which just which is because it's in some ways it's like costs for the for the folks running running the website um and I think at some point we need like some clear language a standard around like web interfaces or any interface in general that's designed for human versus an agent and um it brings me to something that like know I've been thinking about the concept of like agentic noise um and a gentic noise is just thinking through like you know in at some point we'll have like a lot of Agents acting in the digital world on behalf of humans and if these things are not implemented well um they could sort of compete in unusual ways for the human bandwidth and so as as a human you know we all have finite bandwidth and as we navigate the digital world sort of everything is sort of vying or struggling to sort of get a chunk of that bandwidth and I think as humans we want to optimize for like un regretted interactions or in some way that might also translate to interactions with other humans or interactions with like I don't know like just high quality like high quality like artifacts and if we just have like agent sort of lose on the internet um then in some ways they might sort of capture like bandwidth um or I don't know like they might like know capture attention that like is that different or in what ways is that different from like the spam problem right it's like as the cost of sending emails and spending sending text messages you know has dropped to essentially zero now like there's this kind of you know attention tax well uh is is theoretical at least attention tax and our inboxes and our text message boxes you know that kind of Vis for our attention you know and you know per or in line with our previous conversation you know that you know technology has been created to fight that right and so now we don't even think about spam anymore because it's in this you know hinter land in our inbox that we never even bother to check anymore I think the point is we we we need to sort of as we design the agents we need to figure out the technology that like curates our interaction with this agents in following the email email example now we have filters that figure out like what a good email is and what what a bad email is and so we will need technology that sort of fil does the same type of filtering as to um what is a good you know what are good genetic interactions and what are bad ones and and sometimes these things can have like all these other secondary effects right so imagine that like you know if you've if you've interacted with some government websites there's the notion of wily and no to get an appointment to go to some calendar you sort of like click around you get an appointment I imagine like there were a couple of ambitious agents that just went and took up all the appointments now that's a real problem you know like um it's sort of interfering with the social contract that like we have when we interact with all the systems and and we need to figure out ways to evolve evolve and sort of it it might be like some new types of capture just like mention some new types of filtering some new types of humanness confirmation that sort of thing but either way we would need to sort of evolve and get better at that you mentioned the rise of kind of these endtoend gentic benchmarks [Music] um are there and you mentioned I forget the name GA was that the name of the Benchmark that you refer to are there are there frame works or methodologies that you're seeing uh people using to kind of Benchmark their own tasks with you know agent performance on their own tasks as opposed to Benchmark tasks so evaluation is a whole it's a whole it's a whole thing and yeah and so kind of what I'm trying to what I'm trying to ask is like what you know how does all of the kind of energy and and work that's going in eval like apply to agents and in particular like yeah endtoend real world agentic performance yeah um so I think like it whether we like it or not um I think a lot of devaluation here is still still follows the lolm as a judge kind of thing so know you could Benchmark things and some some problems have like objective like results so for on The guia Benchmark um you might have things like you know how long does it take so the answer is a number but then the result that the agent comes I don't even know that I don't go go ahead I I was just gonna say like one thought that I had earlier um really questioned the objectivity of the result like the marathon runner yes like I can divide the speed of the marathon runner by the circumference of the earth but like that gives me one answer but do I expect the agent to take into account wakeful you know time versus sleep time do I take expect the agent to take into account routing do I expect it to take into account Transportation like I don't know that that objective number is actually objective yeah yeah exactly um so so you could have one run where the agent says you know I'm going to assume that we run exactly a wrong around the equator right and we use that exact distance you might have another agent that's a bit more clever and hey know if we look at the map and make estimations around like from this exact point to this other exact point this is the only land travel route complete different answer just like you mentioned we might have another agent that says things like um okay I'm going to take into consideration that half the day this person is going to take a nap um completely different answer and so it becomes very hard to do stuff like that and so the the key Point here is you shouldn't just Benchmark the final answer you should Benchmark the entire trajectory and the best that you can do is to have like an llm as a judge where you define the criteria for evaluation and say it's like it might be something like did this does this result is it based on like solid sound or reasonable assumptions um is the calculation correct it's all this like fuzzy fuzzy logic sort of evaluation criteria that you probably might adopt to sort of Benchmark how um the system behaves yeah it's like really what you're really trying to do is like you're trying to Benchmark its reasoning ability but in a way that I think is different from reasoning benchmarks and that those are all benchmarking those are all comparing against an outcome an answer and what you want to Benchmark is like a thought process it's like a you know an interview where you're given a really hard problem and you're expected to like talk through how you um how you get it that's that's what you want to do and and the the point is not the final thing you come up come up with the the point is where you're thinking in a reasonable logical manner I mean you could put you you could have a couple of like rubrics right so all interviews going the rubric so essentially what you're doing as Elm as a judge is that you're defining the rubrics um and more importantly right how do you interpret the result of these things uh every number itself is not very meaningful but it's relative relative numbers so if you start up with a B version of the system and you get your first set of numbers now what you want is that as you tweak the system the number increases right you don't care about the absolute number you just care about the fact that like no this this this number is indicative of progress in some Direction and essentially it's it's not the number itself that matters it's mostly like as I make changes to the system um do I see changes in a direction that like I care about so I think this is a common thing in this space um and designing the right like structure and as we get like cheaper open source like reasoning models like DSE and Co um we get a chance to be a bit more creative in how we sort of evaluate like you do like L llm as a judge um and I think a lot of people are beginning to sort of integrate like the sort of approaches to their own like like business problems you know invest a bunch of time come up with those rubrics uh structure it well um and then optimize you know I think Jason Le who's also been on the podcast has talked about like you know being a bit creative and coming up with like lowlevel metrics that are not as expensive to compute um also again this this can be like good like relative numbers uh that that you can use to sort of make sense of the direction of of the impact of changes to your systems as you sort of iterate yeah we talked a bit quite a bit about multi-agent systems and some of the you know we talked about architectural considerations and abstractions and um I'm wondering if we've if there are other aspects of that that are are worth digging into I feel like we kind of dug into specifics but we didn't really talk about kind of broad motivation of multi-agent you know and when the complexity of multi-agent systems is warranted from a a use case perspective yeah um yeah so so the idea of multi-agent or autonomous sys it's it's really attractive and one of the downsides is that you might see teams just just hurry just like no rush to try to apply these things even when it might not be the best uh tool for the task of course um choosing what to use should always be a careful scientific process you know like what is your business problem um and does it fit the parameters of the tool so at the end of the day the the focus should always be solving the business problem or user problem now in in determining when to use like an autonomous like multigen system I I have found that like a good framework to use is something I call like the complex task framework and so and know my thesis is that like multi-agent autonomous multi- agent systems are sort of good if your task is complex and what does that mean I think there are four high level areas that I sort of sort of ask people to sort of think through the first is planning hold your task benefits from some sort of just careful planning stage you know can you take the task can you decompose it into a bunch of steps such that like you know successfully completing each step in whatever order will take you from a state of Unsolved to solved now it's your problem fre doesn't have that maybe you don't need a multi an autonomous multi-agent system the second is for each of these steps um does it make sense to sort of um I just step sort of distinct enough that like they benefit from like multiple expertise or tools or specialized knowledge and the idea is that if they do then you can represent each of them as like agents so kind of like domain driven design where let's say if you're writing some piece of software you need like I don't know uh someone that translates the user requirements into a set of like product requirements and then need something some UI engineer that designs the user interface and then you need like some backend API engineer that like creates the back end then they need some software engineer that like builds out the front end you need some integration work and then final and you can think decompose each of these things into steps these are independent expertise these guys can do all their work and you can map them to individual agents another property here is does the task require consuming extensive context again if we look at the software engineering example to I don't know to write code sometimes you might need to read a bunch of documentation you might need to figure out like API references and argument now putting all of that across multiple domains in the same model the same agent might might be challenging because you know we all know about like you know as context just gets long like like LMS might lose context so it just it does make sense to sort of isolate some of that context and all that extensive context processing within in individual agents and I know that like there's all these arguments around like long context and all of that but again does your problem have this parameter and then the final piece is does your problem exist in a dynamic environment so Dynamic here means that let's say you take a step or an action and the environment changes and those changes could lead to errors that you need to then recover from and so um and in that case you need something that can adapt something that can um sort of explore like retries branching and adaptation logic and so across this four like areas planning diverse expertise processing a ton of context and then the task exist in in a dynamic environment requiring adaptation I think if your task does fit like four of these uh things um then maybe like you're probably you've landed with a problem that would really benefit from uh autonomous multi-agent system and are there you know beyond kind of use cases as kind of fitting into those patterns are there specific use cases where you found um folks getting the most bang for their Buck uh with multi-agent there four like high level areas that I've seen a lot of people sort of explore like multiagent systems and these things might not always be like fully autonomous but on the spectrum between like let's say something with some deter some deterministic chain with some complex we try logic it's a little bit of what I was getting at because like for example the canonical example of you know multi-agent system is like a researcher like I'm gonna something's going to go grab some context on the web Something's Gonna Write something Something's Gonna evaluate and edit that and it's going to be a loop but I don't know if that's because that's the best way to build that system or because that's the easiest way to demonstrate that system and there's there's a difference yeah so it's it's on Spectrum so what I'm saying is like you know you know some some complex graph or Loop um and then on on on the far right it's um more autonomous Behavior but software engineering so things like Devon magic code um back office taas tasks or like process automation kind of tasks um legal on finance um customer service and sales agent so these are like uh for high level areas and yesterday just before this call I I sort of pulled the numbers and so I I pulled data from white combinator I mean it's not a perfect representation of everything but it's a good sample and I look for all the companies that mentioned like EI agents explicitly in their task description so in 2022 there were 17 companies that sort of mentioned a agents and in 2024 about 92 companies a 441 increase and across all of these companies you know most of the value proposition that they had was around like the automation of tasks that were previously reliant on human labor things that required a lot of rep repetitive processing things like data analysis things that require communication across multiple systems so the core idea is like if your task you know has repetitive processes we can automate it using LMS or like agentic systems um if your task requires like you know say individuals actual human s of having to coordinate across multiple systems and we can automate some of that and that's how we sort of provide um provide value so um I think it's it's also instructive to sort of look at that list and see what like you know those companies are sort of doing um but um I think this are like we can also categorize all of them into this four high level areas like software engineering um back office tasks in some cases it's like even health or Dentistry Management Systems um in some cases it's just legal like hey know help gather all documents required for your case preparation uh will generate briefs will save your lawyers a lot of money or a lot of time um and in some cases just customer service like we triage all the information around the customer will like come up with automated like resolutions that s thing we have a human in the loop and so these are kind of like what I I am seeing yeah one aspect that you know comes up all the time is like do I need a framework to you know build an agentic system uh you talked already a lot about what the framework is providing uh and um you know a lot of that sounds complex with you know especially when you're talking about uh dealing with the you know challenges of me message passing systems at scale and distributed computing in general um but you know talk us through like you know use a framework versus you know build a yourself um that and that whole thinking yeah um he has a great question and and I like to think back to the early deep learning days you know um say five six seven years ago um Frameworks like tens of flow on P search just coming up and the truth is at the time a sufficiently skilled machine learning engineer could take a model and represented using nonp matrices and did write down the sort of represent their weights using matrices they could write their own like automatic differentiation like library to uh Implement grent desent they could put all of that into like a loop right the training Loop but the problem is that like half the time you make just a single mistake and all the numbers are wrong and it takes weeks or months to have debug that stuff and so as a community you know the machine learning sort of community sort of coales or organize into like good abstractions so for example like we want some good abstractions for automatic differentiation done um we want some good abstractions for a for pass a back backward pass done um we want some good abstractions for like an Optimizer done and it turns out that if you can compose all of these uh abstractions then you can represent almost any neural network uh architecture any type of training Loop and that sort of thing and I feel the same will um same will apply to um to to multi- agent or autonomous agent systems um if you're sufficiently like skilled you probably can write things from scratch and if your if your setup is relatively simple um you just have a simple set of chains you probably don't need a framework however um if you want to build something that's um autonomous and you want to think through you know what is the the control the right control flow um how do we Define when the task is end or it's completed you want to Define you know how do we figure out when to delegate to humans um how do we express what pattern do do do I use um it it gets pretty involved um the the configuration space for the systems um now they sort of interact and they can get uh sort of combinatorial uh in in some sense and and at that point um it's helpful to have framework um so the whole idea is like um so for example like two days ago uh open I released the the operator agent that sort of explos tasks by driving web browsers and with the autogen API the high level API you could Implement about the same functionality in about 40 40 lines of code and so this sort of being able to take building blocks uh sort of put them together um enables like accelerated uh development um and then a lot of known or stabilized patterns just get baked into the library um and so under the the these are like good reasons to like sort of use a framework yeah and and of course one one sorry one one caveat is that like if you use a framework um there's there's some level of Direction and so um Frameworks have defaults they are default system messages um there are some default transformations to the I don't know as message flows through the network and so maybe what hits open AI is really different from let's say the input that the user provided and there might be some some behaviors that you know some some some assumptions that are made on underneath and so from that sense you know know as you use Frameworks it's always a great idea to sort of sort of get familiar with exactly what happens underneath so that like you're debugging process and just making sense of where your and system does is just better and does the do you see the framework moving towards giving the user more visibility and control over some of those underlying uh assumptions and Transformations that you mentioned yeah um so I I think the the right way to go about this is to have two levels of apis and so um have a low-level API where like if people are comfortable expressing just anything they'd like to um it's possible is your core versus chat in the case of autogen yes yes core versus chat in the case of autogen and in situations where in situations where you really really need to to be in control of everything the system does um definitely go with the lowlevel API um and with the higher level API the abstractions um I think a good framework should have um a strong observability story um so first baked into develop experience so for example in autogen there's the idea of as agent sort of interact they sort of yield this asynchronous messages that tell exactly what each of the agents are doing at any specific time and you could take those messages display that in the UI write to some login system in addition to that um we also emit like open Telemetry events down the stack and you could have like your own open Telemetry endpoint uh sync data sync just stole all of that and it's just great as a way to sort of debug and sort of review exactly what went to the API exactly what came back from the API uh just down the stacks I think observability is one way to sort of like open the box and the second has to do with just the flexibility to either use a lowlevel API or high level API got it got it so not necessarily a world in which the developers overriding the system prompt assumptions that the framework is making or th those kinds of things or well well by Design everything is like you can everything is parameterized so for example on the even the agent chat high of API um to Define an agent you can supply your system message directly it's an argument you could Supply the list of tools again an argument the memory interfaces you want this thing to use again an argument and there a bunch of other stuff so they're good defaults but they all can be over reading and again you can also override or overload this classes just classic software engineering and then Implement your own core behaviors so everything is extensible um I was just I was speaking in terms of like the developer that really doesn't want to do anything at all um you you make zero choices about that um and then you have to live with those assumptions um and the observability gives you some visibility into that without you having to necessarily specify everything yep correct so let's jump into your thoughts for 2025 so I think there are a few things you know that I think will happen in 2025 a l of this is informed by my perspective on work done with aogen um so in 2025 I expect to see like um continued Improvement in models I talked about how like you know agents especially in autonomous mode might explore like trajectories that might be suboptimal um that sort of thing I feel like this is a really ripe reinforcement learning trajectory and so let's say you spend help you ask an agent to book a flight um there's a chance that like we figure out ways to collect enough data um do some sort of fine-tuning that gets the agent to just go through the most efficient trajectory to get the work down every single time uh in the first try um I also think the same kind of thing could apply to memory and adaptation and for the most part it's um figuring out like what to remember um what to um what type of um previous experience to retrieve in order to have like a a better chance of solving the currect problem is this kind of like Dynamic optimization of the context is that the way to think about this yeah at a high level yes yeah yeah okay the rack pattern the current R pattern you know it sort of makes all these assumptions that like you have the right in the database and you can sort of retrieve it and use it just in time but there's also the other part of like how do you know when to put things into the memory bank or into the database to your VOR database um does this always have to be explicitly by the user is it something that you can do in some automatic Dynamic manner um and I think it it should be a combination of both um and making this like a reinforcement learning problem one uh would I I I feel this will this will make us like help us make progress um another thing I think another thing that I think will happen in 2025 is the consolidation of a gentic patterns and I talked about like control full patterns and task management patterns and I'm hoping that like we all as a field or a community sort of align well on on what works and the goal is that like it gives us some shared understanding some shared language as to like you know for this class of problems then here's the right pattern for M agent system that works best um I've also been working on the idea of um declarative agents or declarative multi agent systems so imagine that we could 100% specify the entire multi agent system as let's say Json file or something like that and then if we do that then um we can rapid get to a point where um the construction of these systems could even be dynamic so as opposed to developer having to say you know here are like imagine one here's like an orchestrator and four agent and these things are going to it's going to be what solves the task maybe we can even just pop up a level more and whenever we get a task we construct this declarative representation of the entire agent system that might work best for this task and then we sort of instantiate and run this thing and maybe even optimize along the way and then finally I think we'll make progress on the ux for agentic interaction and so a lot of people know have made this drawn this parallel between an agent and let's say junior developer or an intern and so an intern should be proactive is you know and we need that interfaces to do the same so like go get some work done come back and notify the user um they should be interruptable and so know just like on the internet if you get them to if you see they're going down the wrong path you should be able to like sort of interrupt them provide feedback and then get them to keep going um we also talked about how like the a big chunk of the digital world needs to adapt to just agents becoming a part of this interaction it might be agents of txt it might be new types of filters and captures that kind of thing and then there's the whole idea of like figuring out how to show that human bandwidth is not completely overrun by just agents sort of interacting a lot of those point to control of agents in the wild and not necessarily control on the part of the people who are publishing the agents but um other actors in the world yeah yep correct and then there's a final piece that um might not be technology or agent Focus but um there's also the consideration of how the workforce will change as we have more just agents out there and um will there be um will there now a lot of people talk about the concept of hiring agents instead of software Engineers um would we see these sort of parallels some companies advertising that now yeah and what does it mean um of course you know as technology sort of emerges like a lot of things change but I feel like you know at least on the minimum we should be having like a lot of conversation around like how like agents will sort of impact like the software enging field and all of that so yeah so this are like the things I'm thinking of when I think of eii agents in in 2025 do you have a a personal take you know with regard to software Engineers as an example yeah um so I I actually wrote an article about like how AI or AI might impact the software engineering carrier so my the summary of that take um is mostly around it probably will not replace software Engineers one to one because there's a lot of other things software Engineers do um especially senior or both software Engineers there just a lot of things that these Engineers do that are Beyond writing code yes communication and context and translating like human requirements that are usually severely under specified into like software Technical Systems um those things are iterative require a lot of effort back and forth with a human actual human that like yeah might not be too well however um Junior engineering rules like they kind of like hey build a build a web page in react that shows our company's logo or something like that I think jobs like that are gone forever um or things like write a script that like I don't know I mean we we used to have interns I would just write one script that like did one thing um think job like that jobs like that are gone forever um another thing I started to see is like like I've been in teams where in meetings where there are some Engineers that appear more productive and probably more capable than a set of Engineers but essentially what's happening is that these Engineers the first set of Engineers they're just using AI they Le using AI really heavily they have their ID set up they have their workflow set up they figure out how to take problems um write design documents give that to AI get like nice modularized implementations they've learned to write tests they've learned to be really Vigilant about the kind of mistakes that like these models will still make and they've learned to integrate the entire thing into PRS that are error free or bug free that the rest of the team sees now this is a skill you know just with know just with how internet literacy or digital literacy was a skill that everybody needed to cultivate how to use Google search how to navigate the web I feel like software Engineers need to S of invest in that skill that lets them effectively integrate a into your workflow and and the difference is restock so these two groups of Engineers I know for a fact that like they're pretty capable about the same individual capability but just one has invested in just going through that integration process and ensuring they can come out with correct high quality code while the others that just haven't done that yet so that's sort of like my my high level take there won't be like onet to one replacement but some jobs below some level are probably gone and then second like investing in just AI software engineering literacy really creates like significant differences in productivity across Engineers so what do you think Sam yeah you know I I find myself frequently struggling with the you know the classic difficulty seeing exponential change right it's like I I use AI coding agents you you know not professionally because I'm not building any software system you know of any significant scale but I you know I've got you know cursor set up and I use it you know fairly heavily and you know there are times when I think it's like magic and Incredibly productive and I can get so much further so much faster and then there are times when like I get into the loop of like banging my head against this thing and it's not making progress and it's like you know change you know reorganizing the the chairs on the deck of the Titanic like you're asking me to like change things around that have no consequence and you know I think um you know in some ways I think you know it's probably like you know self-driving cars and that you know it's going to take a lot longer than people think because like it's easy to see the 80% progress but the 20% progress is going to take years and years and years and years and years um so uh but I I think what you mentioned I think there's something in what you mentioned in that you know there's a skill to using these systems and it there's not just a skill but I think there's a level of investment in building you know structures around these systems that that I may be I may not see in my personal use but if I'm doing this at the scale of an organization that has thousands of software Engineers I'm able to invest in the degree of customization that you know takes some of the frustration I see out of it I don't I need you know if you know anyone I can talk to to to dig into you know how folks are using this stuff at scale or if anyone um you know has a recommendation for me that's an interview I'd love to do um because I you know the P it's weird because you you on social media it's like yeah yeah it's all the stuff doesn't work like you know and then you know it's like you know they are doomsday you know Dooms doomsayers and cheerleaders and I think you know the reality is in the middle somewhere and um you know certainly what you're saying about skill is an important piece of that yeah um half of the people that like complain that it doesn't work um some I think half of that is a skill issue they just they just haven't come up with like a structure for using these things um if you if you're pretty efficient it's like hey there's a class of problems where don't even bother like don't even bother um and you build intuitions as to like if I get if I ask oh want to do this stuff it's just gonna get confused and reorganize the deck and like or if I ask it to do this without mentioning this really important context it'll completely make a mistake so there there's all the stuff that like I feel some type of tcid knowledge you know there's there's tacit and explicit knowledge so Tass knowledge is like the kind of thing that like it's very you know it but it's really hard to express in words it's like if someone ask you know how do I swim you can't really describe it to them and then they jump in the water and swim um and I feel similarly like just building the right intuition as to when and when not to use it just comes from practice so yeah I think is when to use it when not to use it and I've also found that there's an intuition around when to go to fundamentals meaning you know so I get a lot of value out of um Coen one I'm working with you know libraries or apis that I've not used before and you know I just want to do something quick and dirty I don't necessarily want to go to the docs and read them top to bottom you know but at some point you know you you get a sent and my my ability to sens this has refined over time and so now I'm much quicker to recognize that I'm at this point where I just need to go read the docs and understand what's happening around this this thing that I'm trying to to do because the the model's you know I'm not maybe asking the question the right way or the model hasn't you know seen enough in the training data about this and I need to help it get over the Hub so but that is also a you know a skill or feeling that you know is really useful in this yeah yeah absolutely awesome well Victor it has been wonderful uh both catching up with you personally has been a long time and uh chatting with you about all this stuff uh you know great conversation I really appreciate the time you've taken to go through this with us yeah absolutely thanks for having me Sam pleasure thanks so much [Music] [Music] 
